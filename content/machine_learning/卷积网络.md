---
title: "卷积网络"
date: 2019-10-22T10:10:59+08:00
draft: false
author: "Iven"
weight: 50
---

卷积网络（convolutional network）(LeCun, 1989)，也叫做卷积神经网络（con-volutional neural network, CNN），是一种专门用来处理具有类似网格结构的数据的神经网络。
卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络。卷积网络分为三部分卷积，池化，优化。

<!--more-->

## 卷积

### 卷积运算
卷积运算是将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算），如下图所示：
![卷积运算的方式](/deeplearning/conv_calculate.png)  
**例子：**
![卷积运算的例子](/deeplearning/juanjiyuansaun.png)

卷积通常对应着一个非常稀疏的矩阵（一个几乎所有元素都为零的矩阵）。这是因为核的大小通常要远小于输入图像的大小。任何一个使用矩阵乘法但是并不依赖矩阵结构的特殊性质的神经网络算法，都适用于卷积运算，并且不需要对神经网络做出大的修改。


### 卷积的实现
卷积层 在代码中得实现方式都是将滤波器的应用区域从头开始依次横向展开为1行
```
input_data ― 由（ 数据量，通道，高，长 ）的4维数组构成的输入数据
filter_h ― 滤波器的高
filter_w ― 滤波器的长
stride ― 步幅
pad ― 填充
```
假如有n x n的image,f x f的filter,padding是p,stride是s,则output size是：
```
[(n + 2p -f)/s + 1] x [(n + 2p -f)/s + 1]
```

**基于im2col的展开**
对于输入数据，将应用滤波器的区域（3维方块）横向展开为1列。 im2col 会在所有应用滤波器的地方进行这个展开处理。
![im2col展开](/deeplearning/zhankai.png)
卷积运算的滤波器处理的细节：将滤波器纵向展开为1列，并计算和im2col展开的数据的矩阵乘积，最后转换（reshape）为输出数据的大小
![filter运算](/deeplearning/filter_1.png)
```python
import sys, os
sys.path.append(os.pardir)
from common.util import im2col
x1 = np.random.rand(1, 3, 7, 7)
col1 = im2col(x1, 5, 5, stride=1, pad=0)
print(col1.shape) # (9, 75)
x2 = np.random.rand(10, 3, 7, 7) # 10个数据
col2 = im2col(x2, 5, 5, stride=1, pad=0)
print(col2.shape) # (90, 75)
```

现在使用 im2col 来实现卷积层。这里我们将卷积层实现为名为 Convolution的类。
```python
class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)
        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)
        col = im2col(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T # 滤波器的展开
        out = np.dot(col, col_W) + self.b  #做乘法运算 
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) #重新生成图像 然后转置
        return out
```

{{< hint info >}}
卷积层的初始化方法将滤波器（权重）、偏置、步幅、填充作为参数接收。滤波器是 (FN, C, FH, FW) 的4维形状。另外， FN 、 C 、 FH 、 FW 分别是Filter Number（滤波器数量）、Channel、Filter Height、Filter Width的缩写。  
这里通过 reshape(FN,-1) 将参数指定为 -1 ，这是reshape 的一个便利的功能。通过在 reshape 时指定为 -1 ， reshape 函数会自动计算 -1 维度上的元素个数，以使多维数组的元素个数前后一致。  
比如，(10, 3, 5, 5)形状的数组的元素个数共有750个，指定 reshape(10,-1) 后，就会转换成(10, 75)形状的数组。forward 的实现中，最后会将输出大小转换为合适的形状。转换时使用了NumPy的 transpose 函数。 transpose 会更改多维数组的轴的顺序。通过指定从0开始的索引（编号）序列，就可以更改轴的顺序。
{{< /hint >}}

<br/>

--- 



## 池化

当输入作出少量平移时，池化能够帮助输入的表示近似不变（invariant）。对于平移的不变性是指当我们对输入进行少量平移时，经过池化函数后的大多数输出并不会发生改变。部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置时。例如，当判定一张图像中是否包含人脸时，我们并不需要知道眼睛的精确像素位置，我们只需要知道有一只眼睛在脸的左边，有一只在右边就行了。

![池化层处理方式](/deeplearning/chihuaceng.png)

### 池化层的实现
池化层的实现和卷积层相同，都使用im2col展开输入数据。不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同，池化的应用区域按通道单独展开。
![池化层的实现](/deeplearning/20191031151243.png)

**池化层的特征**
没有要学习的参数:池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。
通道数不发生变化:经过池化运算，输入数据和输出数据的通道数不会发生变化。

代码提示：
```python
class Pooling:
    def __init__(self, pool_h, pool_w, stride=1, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
    def forward(self, x):
        N, C, H, W = x.shape
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)
        # 展开(1)
        col = im2col(x, self.pool_h, 
            self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h*self.pool_w)
        # 最大值(2)
        out = np.max(col, axis=1)
        # 转换(3)
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
        return out
```

## CNN的实现
现在可以根据上面的卷积和池化，再通过神经网络的优化过程就可以实现一个简单的卷积神经了。
![简单的CNN的网络构成](/deeplearning/20191031152535.png)  

{{< expand "参数说明" "..." >}}
input_dim ― 输入数据的维度：（ 通道，高，长 ）  
conv_param ― 卷积层的超参数（字典）。  
字典的关键字如下：  
filter_num ― 滤波器的数量  
filter_size ― 滤波器的大小  
stride ― 步幅  
pad ― 填充  
hidden_size ― 隐藏层（全连接）的神经元数量  
output_size ― 输出层（全连接）的神经元数量  
weitght_int_std ― 初始化时权重的标准差  
{{< /expand >}}

<a href="https://github.com/ivenabc/deeplearning_examples/blob/master/dlfs1/04simple_convnet.py" target="_blank">源码</a>

```python
class SimpleConvNet:
    """
    conv - relu - pool - affine - relu - affine - softmax
    Parameters
    ----------
    input_size : 输入尺寸(MNIST的情况为784)
    hidden_size_list :隐藏层神经元数目列表(e.g. [100, 100, 100])
    output_size : 输出尺寸(MNIST的情况是10)
    activation : 'relu' or 'sigmoid'
    weight_init_std : 指定权重的标准偏差(e.g.0.01)
        当指定“relu”或“he”时，设定“he的初始值”
        在指定“sigmoid”或“xavier”的情况下设定“xavier的初始值”
    """
    def __init__(self, input_dim=(1, 28, 28), 
                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},
                 hidden_size=100, output_size=10, weight_init_std=0.01):
        filter_num = conv_param['filter_num']
        filter_size = conv_param['filter_size']
        filter_pad = conv_param['pad']
        filter_stride = conv_param['stride']
        input_size = input_dim[1]
        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1
        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))
        self.params = {}
        self.params['W1'] = weight_init_std * \
                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)
        self.params['b1'] = np.zeros(filter_num)
        self.params['W2'] = weight_init_std * \
                            np.random.randn(pool_output_size, hidden_size)
        self.params['b2'] = np.zeros(hidden_size)
        self.params['W3'] = weight_init_std * \
                            np.random.randn(hidden_size, output_size)
        self.params['b3'] = np.zeros(output_size)

        self.layers = OrderedDict()
        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],
                                           conv_param['stride'], conv_param['pad'])
        self.layers['Relu1'] = Relu()
        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)
        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])
        self.layers['Relu2'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])

        self.last_layer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.last_layer.forward(y, t)

    def accuracy(self, x, t, batch_size=100):
        if t.ndim != 1 : t = np.argmax(t, axis=1)
        acc = 0.0
        for i in range(int(x.shape[0] / batch_size)):
            tx = x[i*batch_size:(i+1)*batch_size]
            tt = t[i*batch_size:(i+1)*batch_size]
            y = self.predict(tx)
            y = np.argmax(y, axis=1)
            acc += np.sum(y == tt) 
        
        return acc / x.shape[0]

    def numerical_gradient(self, x, t):
        loss_w = lambda w: self.loss(x, t)
        grads = {}
        for idx in (1, 2, 3):
            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])
            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])

        return grads

    def gradient(self, x, t):
        """
        计算梯度(误差反向传播)
        """
        self.loss(x, t)
        dout = 1
        dout = self.last_layer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        grads = {}
        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db
        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

        return grads
        
    def save_params(self, file_name="params.pkl"):
        params = {}
        for key, val in self.params.items():
            params[key] = val
        with open(file_name, 'wb') as f:
            pickle.dump(params, f)

    def load_params(self, file_name="params.pkl"):
        with open(file_name, 'rb') as f:
            params = pickle.load(f)
        for key, val in params.items():
            self.params[key] = val

        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):
            self.layers[key].W = self.params['W' + str(i+1)]
            self.layers[key].b = self.params['b' + str(i+1)]
```