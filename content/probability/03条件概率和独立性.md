### 条件概率

（英语：conditional probability）就是事件A*在事件*B发生的条件下发生的`概率`。条件概率表示为$P(A|B)$，读作“*A*在*B*发生的条件下发生的概率”。

投掷两枚筛子，总共有36种可能，如果现在给定条件，第一个筛子的点数为3，那么投掷两颗筛子的可能结果：(3,1),(3,2),(3,3),(3,4),(3,5),(3,6). 因为每个结果发生的概率都一样。(3,5)的结果就是1/6. 令$P(A)$是第二颗是5的事件，令B是第一颗是三的概率：

则 $P(A|B) = P(AB)/P(B) = 1/36 / 1/6 = 1/6$

### 贝叶斯

要理解贝叶斯推断，必须先理解贝叶斯定理。后者实际上就是计算"条件概率"的公式。

![](/probability/03_ab.png)

根据条件概率可以知道：

$P(A|B) = P(AB)/P(B) $

$P(B|A) = P(BA)/P(A)$

因此可以知道：

$P(A|B)P(B) = P(AB) = P(B|A)P(A)$

$P(A|B)=P(B|A)P(A)/P(B)$

**全概率公式**

![](/probability/03_ABB.png)

由$P(B|A) = P(BA)/P(A)$可知

$P(BA)=P(B|A)P(A)$

所以

$P(B) = P(B|A)P(A) + P(B|A^{c})P(A^{c})$

$P(A|B) = P(B|A)P(A)/(P(B|A)P(A) + P(B|A^{c})P(A^{c}))$

$P(A|B)=P(B|A)P(A)/P(B)$

**贝叶斯的含义**

$P(A|B)=P(A) P(B|A)/P(B)$

我们把P(A)称为"先验概率"（Prior probability），即在B事件发生之前，我们对A事件概率的一个判断。P(A|B)称为"后验概率"（Posterior probability），即在B事件发生之后，我们对A事件概率的重新评估。P(B|A)/P(B)称为"可能性函数"（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。

所以，条件概率可以理解成下面的式子：

> 后验概率　＝　先验概率 ｘ 调整因子

**这就是贝叶斯推断的含义。我们先预估一个"先验概率"，然后加入实验结果，看这个实验到底是增强还是削弱了"先验概率"，由此得到更接近事实的"后验概率"。**

在这里，如果"可能性函数"P(B|A)/P(B)>1，意味着"先验概率"被增强，事件A的发生的可能性变大；如果"可能性函数"=1，意味着B事件无助于判断事件A的可能性；如果"可能性函数"<1，意味着"先验概率"被削弱，事件A的可能性变小。

**贝叶斯列子**

已知某种疾病的发病率是0.001，即1000人中会有1个人得病。现有一种试剂可以检验患者是否得病，它的准确率是0.99，即在患者确实得病的情况下，它有99%的可能呈现阳性。它的误报率是5%，即在患者没有得病的情况下，它有5%的可能呈现阳性。现有一个病人的检验结果为阳性，请问他确实得病的可能性有多大？

假定A事件表示得病，那么P(A)为0.001。这就是"先验概率"，即没有做试验之前，我们预计的发病率。再假定B事件表示阳性，那么要计算的就是P(A|B)。这就是"后验概率"，即做了试验以后，对发病率的估计。

根据条件概率公式，

根据条件概率公式，

![](/probability/03_01.png)

用全概率公式改写分母，

![](/probability/03_02.png)

将数字代入，

![](/probability/03_03.png)

我们得到了一个惊人的结果，P(A|B)约等于0.019。也就是说，即使检验呈现阳性，病人得病的概率，也只是从0.1%增加到了2%左右。这就是所谓的"假阳性"，即阳性结果完全不足以说明病人得病。

### 朴素贝叶斯

假设某个体有n项特征（Feature），分别为F1、F2、...、Fn。现有m个类别（Category），分别为C1、C2、...、Cm。贝叶斯分类器就是计算出概率最大的那个分类，也就是求下面这个算式的最大值：

> 　P(C|F1F2...Fn)  
> 　　= P(F1F2...Fn|C)P(C) / P(F1F2...Fn)

由于 P(F1F2...Fn) 对于所有的类别都是相同的，可以省略，问题就变成了求

> 　P(F1F2...Fn|C)P(C)

的最大值。

朴素贝叶斯分类器则是更进一步，假设所有特征都彼此独立，因此

> 　P(F1F2...Fn|C)P(C)  
> 　　= P(F1|C)P(F2|C) ... P(Fn|C)P(C)

上式等号右边的每一项，都可以从统计资料中得到，由此就可以计算出每个类别对应的概率，从而找出最大概率的那个类。

虽然"所有特征彼此独立"这个假设，在现实中不太可能成立，但是它可以大大简化计算，而且有研究表明对分类结果的准确性影响不大。

朴素贝叶斯例子

下面是一组人类身体特征的统计资料。

> 　　性别　　身高（英尺）　体重（磅）　　脚掌（英寸）
> 
> 　　男 　　　6 　　　　　　180　　　　　12  
> 　　男 　　　5.92　　　　　190　　　　　11  
> 　　男 　　　5.58　　　　　170　　　　　12  
> 　　男 　　　5.92　　　　　165　　　　　10  
> 　　女 　　　5 　　　　　　100　　　　　6  
> 　　女 　　　5.5 　　　　　150　　　　　8  
> 　　女 　　　5.42　　　　　130　　　　　7  
> 　　女 　　　5.75　　　　　150　　　　　9

已知某人身高6英尺、体重130磅，脚掌8英寸，请问该人是男是女？

根据朴素贝叶斯分类器，计算下面这个式子的值。

> P(身高|性别) x P(体重|性别) x P(脚掌|性别) x P(性别)

这里的困难在于，由于身高、体重、脚掌都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办？

这时，可以假设男性和女性的身高、体重、脚掌都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。

比如，男性的身高是均值5.855、方差0.035的正态分布。所以，男性的身高为6英尺的概率的相对值等于1.5789（大于1并没有关系，因为这里是密度函数的值，只用来反映各个值的相对可能性）。

![](/probability/03_04.png)

有了这些数据以后，就可以计算性别的分类了。

> 　　P(身高=6|男) x P(体重=130|男) x P(脚掌=8|男) x P(男)  
> 　　　　= 6.1984 x e-9
> 
> 　　P(身高=6|女) x P(体重=130|女) x P(脚掌=8|女) x P(女)  
> 　　　　= 5.3778 x e-4

可以看到，女性的概率比男性要高出将近10000倍，所以判断该人为女性。


