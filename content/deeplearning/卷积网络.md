---
title: "卷积网络"
date: 2019-10-22T10:10:59+08:00
draft: true
---

卷积网络（convolutional network）(LeCun, 1989)，也叫做卷积神经网络（con-volutional neural network, CNN），是一种专门用来处理具有类似网格结构的数据的神经网络。
卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络。

## 卷积

### 卷积运算的方式
将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）
![卷积运算的方式](/deeplearning/conv_calculate.png)
例子：
![卷积运算的例子](/deeplearning/juanjiyuansaun.png)

卷积通常对应着一个非常稀疏的矩阵（一个几乎所有元素都为零的矩阵）。这是因为核的大小通常要远小于输入图像的大小。任何一个使用矩阵乘法但是并不依赖矩阵结构的特殊性质的神经网络算法，都适用于卷积运算，并且不需要对神经网络做出大的修改。


### 卷积的实现
卷积层 在代码中得实现方式都是将滤波器的应用区域从头开始依次横向展开为1行
```
input_data ― 由（ 数据量，通道，高，长 ）的4维数组构成的输入数据
filter_h ― 滤波器的高
filter_w ― 滤波器的长
stride ― 步幅
pad ― 填充
```
假如有n x n的image,f x f的filter,padding是p,stride是s,则output size是：
```
[(n + 2p -f)/s + 1] x [(n + 2p -f)/s + 1]
```

**基于im2col的展开**
对于输入数据，将应用滤波器的区域（3维方块）横向展开为1列。 im2col 会在所有应用滤波器的地方进行这个展开处理。
![im2col展开](/deeplearning/zhankai.png)
卷积运算的滤波器处理的细节：将滤波器纵向展开为1列，并计算和im2col展开的数据的矩阵乘积，最后转换（reshape）为输出数据的大小
![filter运算](/deeplearning/filter_1.png)
```python
import sys, os
sys.path.append(os.pardir)
from common.util import im2col
x1 = np.random.rand(1, 3, 7, 7)
col1 = im2col(x1, 5, 5, stride=1, pad=0)
print(col1.shape) # (9, 75)
x2 = np.random.rand(10, 3, 7, 7) # 10个数据
col2 = im2col(x2, 5, 5, stride=1, pad=0)
print(col2.shape) # (90, 75)
```

现在使用 im2col 来实现卷积层。这里我们将卷积层实现为名为 Convolution的类。
```python
class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)
        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)
        col = im2col(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T # 滤波器的展开
        out = np.dot(col, col_W) + self.b
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)
        return out
```

<br>

## 池化

当输入作出少量平移时，池化能够帮助输入的表示近似不变（invariant）。对于平移的不变性是指当我们对输入进行少量平移时，经过池化函数后的大多数输出并不会发生改变。部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置时。例如，当判定一张图像中是否包含人脸时，我们并不需要知道眼睛的精确像素位置，我们只需要知道有一只眼睛在脸的左边，有一只在右边就行了。

![池化层处理方式](/deeplearning/chihuaceng.png)

**池化层的特征**
没有要学习的参数:池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。
通道数不发生变化:经过池化运算，输入数据和输出数据的通道数不会发生变化。

代码提示：
```python
class Pooling:
    def __init__(self, pool_h, pool_w, stride=1, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
    def forward(self, x):
        N, C, H, W = x.shape
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)
        # 展开(1)
        col = im2col(x, self.pool_h, 
            self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h*self.pool_w)
        # 最大值(2)
        out = np.max(col, axis=1)
        # 转换(3)
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
        return out
```

### CNN的实现

参数说明

- input_dim ― 输入数据的维度：（ 通道，高，长 ）
- conv_param ― 卷积层的超参数（字典）。
字典的关键字如下：
filter_num ― 滤波器的数量
filter_size ― 滤波器的大小
stride ― 步幅
pad ― 填充

- hidden_size ― 隐藏层（全连接）的神经元数量
- output_size ― 输出层（全连接）的神经元数量
- weitght_int_std ― 初始化时权重的标准差

代码实现：
```python
class SimpleConvNet:
    def __init__(self, input_dim=(1, 28, 28),
        conv_param={'filter_num':30, 'filter_size':5,
        'pad':0, 'stride':1},
        hidden_size=100, output_size=10, weight_init_std=0.01):
        filter_num = conv_param['filter_num']
        filter_size = conv_param['filter_size']
        filter_pad = conv_param['pad']
        filter_stride = conv_param['stride']
        input_size = input_dim[1]
        conv_output_size = (input_size - filter_size + 2*filter_pad) / \
        filter_stride + 1
        pool_output_size = int(filter_num * (conv_output_size/2) *
        (conv_output_size/2))
```