---
title: "卷积网络"
date: 2019-10-22T10:10:59+08:00
draft: true
---

卷积网络（convolutional network）(LeCun, 1989)，也叫做卷积神经网络（con-volutional neural network, CNN），是一种专门用来处理具有类似网格结构的数据的神经网络。
卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络。

## 卷积运算

### 卷积运算的方式
将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为乘积累加运算）
![卷积运算的方式](/deeplearning/conv_calculate.png)

![卷积运算的例子](/deeplearning/juanjiyuansaun.png)

卷积通常对应着一个非常稀疏的矩阵（一个几乎所有元素都为零的矩阵）。这是因为核的大小通常要远小于输入图像的大小。任何一个使用矩阵乘法但是并不依赖矩阵结构的特殊性质的神经网络算法，都适用于卷积运算，并且不需要对神经网络做出大的修改。

### 卷积的意义
卷积运算通过三个重要的思想来帮助改进机器学习系统：**稀疏交互（sparseinteractions）、参数共享（parameter sharing）、等变表示（equivariant representa-tions）**。另外，卷积提供了一种处理大小可变的输入的方法。

传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参数矩阵中每一个单独的参数都描述了一个输入单元与一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。然而，卷积网络具有稀疏交互（sparse interactions）（也叫做稀疏连接（sparse connectivity）或者稀疏权重（sparse weights））的特征。这是使核的大小远小于输入的大小来达到的。举个例子，当处理一张图像时，输入的图像可能包含成千上万个像素点，但是我们可以通过只占用几十到上百个像素点的核来检测一些小的有意义的特征，例如图像的边缘。这意味着我们需要存储的参数更少，不仅减少了模型的存储需求，而且提高了它的统计效率。

参数共享（parameter sharing）是指在一个模型的多个函数中使用相同的参数。在传统的神经网络中，当计算一层的输出时，权重矩阵的每一个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。作为参数共享的同义词，我们可以说一个网络含有绑定的权重（tied weights），因为用于一个输入的权重也会被绑定在其他的权重上。在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（是否考虑边界像素取决于对边界决策的设计）。

对于卷积，参数共享的特殊形式使得神经网络层具有对平移等变（equivariance）的性质。如果一个函数满足输入改变，输出也以同样的方式改变这一性质，我们就说它是等变 (equivariant) 的。特别地，如果函数 f(x) 与 g(x) 满足 f(g(x)) = g(f(x))，我们就说 f(x) 对于变换 g 具有等变性。对于卷积来说，如果令 g 是输入的任意平移函数，那么卷积函数对于 g 具有等变性。


<br>

## 池化

![池化层处理方式](/deeplearning/chihuaceng.png)